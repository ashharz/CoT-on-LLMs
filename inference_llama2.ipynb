{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TvyCtCmmLme-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710504615342,
          "user_tz": -330,
          "elapsed": 3905,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "TvyCtCmmLme-",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "7UfqKQiCDVZkcG1RdcQp6loq",
      "metadata": {
        "tags": [],
        "id": "7UfqKQiCDVZkcG1RdcQp6loq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710504619263,
          "user_tz": -330,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer,AutoConfig,AutoModelForCausalLM\n",
        "import gc\n",
        "\n",
        "# !pip install accelerate"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "token = \"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\"\n",
        "\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-64g-actorder_True\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision = \"gptq-4bit-32g-actorder_True\",\n",
        "                                              token=token)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.bos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "model.config.pad_token_id = tokenizer.bos_token_id\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dwOxbfGpNX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710504642718,
          "user_tz": -330,
          "elapsed": 23456,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f4d3bf11-6b88-4ffe-9a55-cea52af58181"
      },
      "id": "82dwOxbfGpNX",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "          (k_proj): QuantLinear()\n",
              "          (o_proj): QuantLinear()\n",
              "          (q_proj): QuantLinear()\n",
              "          (v_proj): QuantLinear()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (act_fn): SiLUActivation()\n",
              "          (down_proj): QuantLinear()\n",
              "          (gate_proj): QuantLinear()\n",
              "          (up_proj): QuantLinear()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompts_and_answers(file_path):\n",
        "    df = pd.read_csv(file_path,index_col=0)\n",
        "    return list(df[\"0\"]),list(df[\"1\"])"
      ],
      "metadata": {
        "id": "du2LeAhRWdbf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710504642719,
          "user_tz": -330,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "du2LeAhRWdbf",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 1319\n",
        "n = 200\n",
        "bs = 10\n",
        "for full_prompt in [\"std\"]:\n",
        "  for n_shot in range(3,9):\n",
        "    file_path = f\"./gcp_data_inp_gsm8k/{full_prompt}-{n_shot}-{n_examples}.csv\"\n",
        "    print(f\"processing {file_path}\")\n",
        "    prompts,answers = get_prompts_and_answers(file_path)\n",
        "    prompts,answers = prompts[:n],answers[:n]\n",
        "\n",
        "    final_outputs = []\n",
        "    for i in range(0,len(prompts),bs):\n",
        "      j = len(prompts) if i+bs>len(prompts) else i+bs\n",
        "      tokens = tokenizer(prompts[i:j],padding=True,return_tensors=\"pt\").to(\"cuda\")\n",
        "      output_ids = model.generate(**tokens,max_new_tokens=120)\n",
        "      output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "      final_outputs.extend(output)\n",
        "      if i%5==0:\n",
        "        print(f\"{i}\",end=\" \")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    file_path = f\"./gcp_data_out_gsm8k/{full_prompt}-{n_shot}-{n_examples}.csv\"\n",
        "    pd.DataFrame({\"0\":final_outputs,\"1\":answers}).to_csv(file_path)\n",
        "    print(f\"\\nSaved {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AtHMUi4XwuH",
        "outputId": "03719b2d-a063-481d-c031-64bd0005d53f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710494050219,
          "user_tz": -330,
          "elapsed": 10,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "0AtHMUi4XwuH",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing ./gcp_data_inp_gsm8k/std-3-1319.csv\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 \n",
            "Saved ./gcp_data_out_gsm8k/std-3-1319.csv\n",
            "processing ./gcp_data_inp_gsm8k/std-4-1319.csv\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 \n",
            "Saved ./gcp_data_out_gsm8k/std-4-1319.csv\n",
            "processing ./gcp_data_inp_gsm8k/std-5-1319.csv\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 \n",
            "Saved ./gcp_data_out_gsm8k/std-5-1319.csv\n",
            "processing ./gcp_data_inp_gsm8k/std-6-1319.csv\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 \n",
            "Saved ./gcp_data_out_gsm8k/std-6-1319.csv\n",
            "processing ./gcp_data_inp_gsm8k/std-7-1319.csv\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 \n",
            "Saved ./gcp_data_out_gsm8k/std-7-1319.csv\n",
            "processing ./gcp_data_inp_gsm8k/std-8-1319.csv\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 \n",
            "Saved ./gcp_data_out_gsm8k/std-8-1319.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = f\"./gcp_data_out_gsm8k/{full_prompt}-{n_shot}-{n_examples}.csv\"\n",
        "pd.DataFrame({\"0\":final_outputs,\"1\":answers[:len(final_outputs)]}).to_csv(file_path)"
      ],
      "metadata": {
        "id": "oHBHFU1WIY_W",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710489844460,
          "user_tz": -330,
          "elapsed": 339,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "oHBHFU1WIY_W",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gcp_data_inp_aqua.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlgU3npILAhm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710494151415,
          "user_tz": -330,
          "elapsed": 310,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4c3e3538-3df5-43fb-b5d1-9efacd89fa95"
      },
      "id": "vlgU3npILAhm",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gcp_data_inp_aqua.zip\n",
            "  inflating: gcp_data_inp_aqua/cot-0-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/cot-1-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/cot-2-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/cot-3-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/cot-4-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/std-0-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/std-1-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/std-2-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/std-3-254.csv  \n",
            "  inflating: gcp_data_inp_aqua/std-4-254.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_examples = 254\n",
        "bs = 5\n",
        "for full_prompt in [\"std\"]:\n",
        "  for n_shot in range(0,5):\n",
        "    file_path = f\"./gcp_data_inp_aqua/{full_prompt}-{n_shot}-{n_examples}.csv\"\n",
        "    print(f\"processing {file_path}\")\n",
        "    prompts,answers = get_prompts_and_answers(file_path)\n",
        "\n",
        "    final_outputs = []\n",
        "    for i in range(0,len(prompts),bs):\n",
        "      j = len(prompts) if i+bs>len(prompts) else i+bs\n",
        "      tokens = tokenizer(prompts[i:j],padding=True,return_tensors=\"pt\").to(\"cuda\")\n",
        "      output_ids = model.generate(**tokens,max_new_tokens=512)\n",
        "      output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "      final_outputs.extend(output)\n",
        "      if i%5==0:\n",
        "        print(f\"{i}\",end=\" \")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    file_path = f\"./gcp_data_out_aqua/{full_prompt}-{n_shot}-{n_examples}.csv\"\n",
        "    pd.DataFrame({\"0\":final_outputs,\"1\":answers}).to_csv(file_path)\n",
        "    print(f\"\\nSaved {file_path}\")"
      ],
      "metadata": {
        "id": "kcY_8UggfSCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710510081144,
          "user_tz": -330,
          "elapsed": 1498603,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "14f27258-4e6c-425f-f293-fc7ceca128c6"
      },
      "id": "kcY_8UggfSCb",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing ./gcp_data_inp_aqua/std-0-254.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 \n",
            "Saved ./gcp_data_out_aqua/std-0-254.csv\n",
            "processing ./gcp_data_inp_aqua/std-1-254.csv\n",
            "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 \n",
            "Saved ./gcp_data_out_aqua/std-1-254.csv\n",
            "processing ./gcp_data_inp_aqua/std-2-254.csv\n",
            "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 \n",
            "Saved ./gcp_data_out_aqua/std-2-254.csv\n",
            "processing ./gcp_data_inp_aqua/std-3-254.csv\n",
            "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 \n",
            "Saved ./gcp_data_out_aqua/std-3-254.csv\n",
            "processing ./gcp_data_inp_aqua/std-4-254.csv\n",
            "0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 205 210 215 220 225 230 235 240 245 250 \n",
            "Saved ./gcp_data_out_aqua/std-4-254.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfbNbke-cRM0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1710495026867,
          "user_tz": -330,
          "elapsed": 262,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e1ad7923-d0cc-4606-e9fd-6c248c85c56e"
      },
      "id": "WfbNbke-cRM0",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]A car is being driven , in a straight line and at a uniform speed , towards the base of a vertical tower . The top of the tower is observed from the car and , in the process , it takes 10 minutes for the angle of elevation to change from 45 ° to 60 ° . After how much more time will this car reach the base of the tower ?\n",
            "Answer Choices:\n",
            "A) 5 ( √ 3 + 1)\n",
            "B) 6 ( √ 3 + √ 2)\n",
            "C) 7 ( √ 3 – 1)\n",
            "D) 8 ( √ 3 – 2)\n",
            "E) None of these[/INST]  To solve this problem, we need to use the concept of similar triangles.\n",
            "The angle of elevation of the tower is increasing from 45° to 60°, which means that the angle between the line of sight and the base of the tower is also increasing. Since the car is moving in a straight line, the angle between the line of sight and the base of the tower is constant.\n",
            "Let's call the distance from the base of the tower to the point where the angle of elevation changes from 45° to 60° as x.\n",
            "Using the concept of similar triangles, we can draw a triangle with the base as the distance from the base of the tower to the point where the angle of elevation changes, and the height as the distance from the point where the angle of elevation changes to the top of the tower.\n",
            "The height of the tower is given as √3 times the distance from the base to the point where the angle of elevation changes. So, the height of the tower is:\n",
            "H = √3x\n",
            "\n",
            "The time taken for the angle of elevation to change from 45° to 60° is 10 minutes. Since the car is moving at a uniform speed, the distance traveled by the car during this time is also constant. Let's call this distance as y.\n",
            "We can now use the concept of similar triangles to find the time taken for the car to reach the base of the tower.\n",
            "The distance traveled by the car is equal to the height of the tower times the cosine of the angle of elevation:\n",
            "y = H \\* cos(60° - 45°)\n",
            "= H \\* cos(15°)\n",
            "= H \\* 0.5\n",
            "= √3x \\* 0.5\n",
            "= x/2\n",
            "\n",
            "Therefore, the car will reach the base of the tower in 2x/2 = x minutes.\n",
            "So, the answer is (E) None of these.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL float16 very slow ~2 token/sec\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# model_name_7b_chat = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# token = \"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\"\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name_7b_chat,device_map=\"auto\",torch_dtype=torch.bfloat16,token=token)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name_7b_chat,padding_side=\"left\",token=token)\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.bos_token\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# model.config.pad_token_id = tokenizer.bos_token_id\n",
        "# model = model.bfloat16()\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "xYSZj-AbLQGS",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1710489686973,
          "user_tz": -330,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "xYSZj-AbLQGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwtbCF4TkWJx"
      },
      "id": "RwtbCF4TkWJx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}